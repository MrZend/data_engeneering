{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7432b87b-94c6-4122-8a50-a8882251abce",
   "metadata": {},
   "source": [
    "## Install neccessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c180d740-39aa-4a19-8e11-21a7571ab60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting delta-spark==3.2.0\n",
      "  Downloading delta_spark-3.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: pyspark<3.6.0,>=3.5.0 in /usr/local/spark/python (from delta-spark==3.2.0) (3.5.0)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from delta-spark==3.2.0) (6.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata>=1.0.0->delta-spark==3.2.0) (3.17.0)\n",
      "Collecting py4j==0.10.9.7 (from pyspark<3.6.0,>=3.5.0->delta-spark==3.2.0)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading delta_spark-3.2.0-py3-none-any.whl (21 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: py4j, delta-spark\n",
      "Successfully installed delta-spark-3.2.0 py4j-0.10.9.7\n"
     ]
    }
   ],
   "source": [
    "!pip install delta-spark==3.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f86fcf-9d6f-4090-830b-9feaedff80f7",
   "metadata": {},
   "source": [
    "## Work with DeltaLake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa2a92f-ff53-4188-b85f-36123f7c822f",
   "metadata": {},
   "source": [
    "### Import neccessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "82c14eb8-14f8-4ad7-998c-8b80a74ae897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "from pyspark.sql.functions import round\n",
    "from delta import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2c199d-ba10-4d7f-845b-49f41ab49bae",
   "metadata": {},
   "source": [
    "### Initialize and set up spark to work with DeltaLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7ebea19b-e05f-4351-bf56-8efc0537dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = SparkSession.builder.appName(\"Delta Lake Lab7\") \\\n",
    "                              .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "                              .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "                              .config(\"spark.sql.catalog.my_catalog.use-nullable-query-schema\", \"false\") \\\n",
    "                              .config(\"spark.databricks.delta.schema.autoMerge.enabled\", \"false\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcd38d4-a23c-4e2b-af8f-149e59eea77e",
   "metadata": {},
   "source": [
    "### Create schema of data and put first data to DeltaLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "741dc4cf-8b2c-4183-b142-42a3bb3798ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Чомусь обмеження nullable не додавались в DeltaLake при додаванні даних в нього через DataFrame sparka, тому обхідним шляхом створюємо схему даних на пряму\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE delta.`/tmp/delta-traffic` (\n",
    "  site_id INTEGER NOT NULL,\n",
    "  visits INTEGER NOT NULL,\n",
    "  avg_duration FLOAT NOT NULL,\n",
    "  traffic_source STRING NOT NULL\n",
    ") USING DELTA;\n",
    "\"\"\")\n",
    "\n",
    "# Update. Під час оновлення існуючих записів (наприклад, оновити кількість визитів та середній час перегляду) виникла проблема,\n",
    "# в тому що ми не можемо однозначно ідентифікувати певний запис, оскільки можливо таке, що на певному сайті 100% можуть бути декілька джерел трафіку\n",
    "# Тому було вирішено зробити поле traffic_source також Not Null, і це поле буде виступати також Первинним ключем в складеному ключі"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9ba4b878-f923-4b03-863b-b79713847eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+--------------+\n",
      "|site_id|visits|avg_duration|traffic_source|\n",
      "+-------+------+------------+--------------+\n",
      "|      1|    50|         3.5|        direct|\n",
      "|      1|   100|         3.7|      referral|\n",
      "|      1|    75|         4.1|     instagram|\n",
      "|      1|   228|         6.4|           ads|\n",
      "|      2|   500|        10.1|        direct|\n",
      "|      2|    43|        15.3|      referral|\n",
      "|      2|   101|         7.3|     instagram|\n",
      "|      2|    82|         6.8|           ads|\n",
      "+-------+------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Створення схеми даних\n",
    "schema = StructType([\n",
    "    StructField(\"site_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"visits\", IntegerType(), nullable=False),\n",
    "    StructField(\"avg_duration\", FloatType(), nullable=False),\n",
    "    StructField(\"traffic_source\", StringType(), nullable=False)\n",
    "])\n",
    "\n",
    "# Приклад даних для моніторингу трафіку веб-сторінки\n",
    "data_1 = [\n",
    "    (1, 50, 3.5, \"direct\"),\n",
    "    (1, 100, 3.7, \"referral\"),\n",
    "    (1, 75, 4.1, \"instagram\"),\n",
    "    (1, 228, 6.4, \"ads\"),\n",
    "    (2, 500, 10.1, \"direct\"),\n",
    "    (2, 43, 15.3, \"referral\"),\n",
    "    (2, 101, 7.3, \"instagram\"),\n",
    "    (2, 82, 6.8, \"ads\")\n",
    "]\n",
    "\n",
    "# Створення DataFrame з попередньо ініцілізованих даних\n",
    "df = spark.createDataFrame(data_1, schema=schema)\n",
    "df.show()\n",
    "\n",
    "# Збереження даних у форматі Delta\n",
    "df.write.format(\"delta\").mode(\"append\").save(\"/tmp/delta-traffic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b1fea5b7-4645-4648-a85a-baab7651588f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+--------------+\n",
      "|site_id|visits|avg_duration|traffic_source|\n",
      "+-------+------+------------+--------------+\n",
      "|      1|    75|         4.1|     instagram|\n",
      "|      2|   101|         7.3|     instagram|\n",
      "|      2|    43|        15.3|      referral|\n",
      "|      1|   100|         3.7|      referral|\n",
      "|      2|   500|        10.1|        direct|\n",
      "|      1|    50|         3.5|        direct|\n",
      "|      2|    82|         6.8|           ads|\n",
      "|      1|   228|         6.4|           ads|\n",
      "+-------+------+------------+--------------+\n",
      "\n",
      "root\n",
      " |-- site_id: integer (nullable = false)\n",
      " |-- visits: integer (nullable = false)\n",
      " |-- avg_duration: float (nullable = false)\n",
      " |-- traffic_source: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Читання даних з таблиці Delta\n",
    "delta_table = spark.read.format(\"delta\").load(\"/tmp/delta-traffic\")\n",
    "delta_table.show()\n",
    "delta_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "424fd332-989c-4c69-8ffa-a5e46c7614d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkValueError",
     "evalue": "[CANNOT_BE_NONE] Argument `obj` can not be None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkValueError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 16\u001b[0m\n\u001b[1;32m     10\u001b[0m data_none_3 \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     11\u001b[0m     (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m82\u001b[39m, \u001b[38;5;241m6.8\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mads\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m ]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#df_none_1 = spark.createDataFrame(data_none_1, schema=schema)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#df_none_2 = spark.createDataFrame(data_none_2, schema=schema)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m df_none_3 \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_none_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[0;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1090\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# make sure data could consumed multiple times\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m-> 1090\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m   1093\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferSchemaFromList(data, names\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1459\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe.<locals>.prepare\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(obj):\n\u001b[0;32m-> 1459\u001b[0m     \u001b[43mverify_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:2187\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[0;32m-> 2187\u001b[0m         \u001b[43mverify_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:2160\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_struct\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2150\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m   2151\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLENGTH_SHOULD_BE_THE_SAME\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2152\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2157\u001b[0m             },\n\u001b[1;32m   2158\u001b[0m         )\n\u001b[1;32m   2159\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v, (_, verifier) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(obj, verifiers):\n\u001b[0;32m-> 2160\u001b[0m         \u001b[43mverifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2162\u001b[0m     d \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:2186\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mverify_nullability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   2187\u001b[0m         verify_value(obj)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:1989\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_nullability\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1987\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1988\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1989\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m   1990\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_BE_NONE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1991\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m   1992\u001b[0m         )\n\u001b[1;32m   1993\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1994\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mPySparkValueError\u001b[0m: [CANNOT_BE_NONE] Argument `obj` can not be None."
     ]
    }
   ],
   "source": [
    "# Приклад додавання даних з Nullом\n",
    "data_none_1 = [\n",
    "    (3, None, 6.8, \"ads\")\n",
    "]\n",
    "\n",
    "data_none_2 = [\n",
    "    (3, 82, None, \"ads\")\n",
    "]\n",
    "\n",
    "data_none_3 = [\n",
    "    (None, 82, 6.8, \"ads\")\n",
    "]\n",
    "\n",
    "#df_none_1 = spark.createDataFrame(data_none_1, schema=schema)\n",
    "#df_none_2 = spark.createDataFrame(data_none_2, schema=schema)\n",
    "df_none_3 = spark.createDataFrame(data_none_3, schema=schema)\n",
    "\n",
    "# Навіть створити DataFrame з None значенням не можемо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "23413fd3-5e30-41e3-940a-3838fb97e097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+--------------+\n",
      "|site_id|visits|avg_duration|traffic_source|\n",
      "+-------+------+------------+--------------+\n",
      "|      1|   228|         6.4|           ads|\n",
      "|      2|   120|         7.1|     instagram|\n",
      "|      2|    95|         6.4|           ads|\n",
      "|      3|     5|         3.2|        direct|\n",
      "+-------+------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Приклад оновлення та додавання нових даних\n",
    "data_2 = [\n",
    "    (1, 228, 6.4, \"ads\"),\n",
    "    (2, 120, 7.1, \"instagram\"),\n",
    "    (2, 95, 6.4, \"ads\"),\n",
    "    (3, 5, 3.2, \"direct\")\n",
    "]\n",
    "\n",
    "df_2 = spark.createDataFrame(data_2, schema=schema)\n",
    "df_2.show()\n",
    "\n",
    "# Створюємо обʼєкт DeltaTable з існуючої таблиці для того, щоб провести оновлення даних за допомогою мерджу\n",
    "delta_table_2 = DeltaTable.forPath(spark, \"/tmp/delta-traffic\")\n",
    "\n",
    "# Проводемо оновлення даних за допомогою мерджу\n",
    "delta_table_2.alias(\"web_traffics\").merge(\n",
    "        df_2.alias(\"new_data\"),\n",
    "            \"web_traffics.site_id = new_data.site_id AND web_traffics.traffic_source = new_data.traffic_source\"\n",
    "        ) \\\n",
    "        .whenMatchedUpdate(set={\n",
    "            \"visits\": \"new_data.visits\",\n",
    "            \"avg_duration\": \"new_data.avg_duration\"\n",
    "        }) \\\n",
    "        .whenNotMatchedInsertAll() \\\n",
    "        .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3ac8c547-a415-4b95-b8b8-37c476e30942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+--------------+\n",
      "|site_id|visits|avg_duration|traffic_source|\n",
      "+-------+------+------------+--------------+\n",
      "|      1|   228|         6.4|           ads|\n",
      "|      2|    95|         6.4|           ads|\n",
      "|      2|   120|         7.1|     instagram|\n",
      "|      3|     5|         3.2|        direct|\n",
      "|      1|    75|         4.1|     instagram|\n",
      "|      2|    43|        15.3|      referral|\n",
      "|      1|   100|         3.7|      referral|\n",
      "|      2|   500|        10.1|        direct|\n",
      "|      1|    50|         3.5|        direct|\n",
      "+-------+------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Перевіряємо чи дані успішно оновилися\n",
    "delta_table_2_df = spark.read.format(\"delta\").load(\"/tmp/delta-traffic\")\n",
    "delta_table_2_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5b61d8f2-103a-4ef8-86af-8c7da6cd2aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|           operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      2|2025-01-04 23:39:...|  NULL|    NULL|               MERGE|{predicate -> [\"(...|NULL|    NULL|     NULL|          1|  Serializable|        false|{numTargetRowsCop...|        NULL|Apache-Spark/3.5....|\n",
      "|      1|2025-01-04 23:38:...|  NULL|    NULL|               WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          0|  Serializable|         true|{numFiles -> 9, n...|        NULL|Apache-Spark/3.5....|\n",
      "|      0|2025-01-04 23:38:...|  NULL|    NULL|CREATE OR REPLACE...|{partitionBy -> [...|NULL|    NULL|     NULL|       NULL|  Serializable|         true|                  {}|        NULL|Apache-Spark/3.5....|\n",
      "+-------+--------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Перегляд таблиці з історією версій даних\n",
    "history = delta_table_2.history()\n",
    "history.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "779e8737-5257-4463-a557-f18c3d30e5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+--------------+\n",
      "|site_id|visits|avg_duration|traffic_source|\n",
      "+-------+------+------------+--------------+\n",
      "|      1|    75|         4.1|     instagram|\n",
      "|      2|   101|         7.3|     instagram|\n",
      "|      2|    43|        15.3|      referral|\n",
      "|      1|   100|         3.7|      referral|\n",
      "|      2|   500|        10.1|        direct|\n",
      "|      1|    50|         3.5|        direct|\n",
      "|      2|    82|         6.8|           ads|\n",
      "|      1|   228|         6.4|           ads|\n",
      "+-------+------+------------+--------------+\n",
      "\n",
      "Нові записи у версії 2:\n",
      "+-------+--------------+------+------------+\n",
      "|site_id|traffic_source|visits|avg_duration|\n",
      "+-------+--------------+------+------------+\n",
      "|      3|        direct|     5|         3.2|\n",
      "+-------+--------------+------+------------+\n",
      "\n",
      "Тенденція для існуючих записів:\n",
      "+-------+--------------+------------+-------------------+\n",
      "|site_id|traffic_source|visit_change|avg_duration_change|\n",
      "+-------+--------------+------------+-------------------+\n",
      "|      1|     instagram|           0|                0.0|\n",
      "|      2|     instagram|          19|               -0.2|\n",
      "|      2|      referral|           0|                0.0|\n",
      "|      1|      referral|           0|                0.0|\n",
      "|      2|        direct|           0|                0.0|\n",
      "|      1|        direct|           0|                0.0|\n",
      "|      2|           ads|          13|               -0.4|\n",
      "|      1|           ads|           0|                0.0|\n",
      "+-------+--------------+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Повернення до попередньої версії\n",
    "df_version1 = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"/tmp/delta-traffic\")\n",
    "df_version2 = spark.read.format(\"delta\").option(\"versionAsOf\", 2).load(\"/tmp/delta-traffic\")\n",
    "\n",
    "df_version1.show()\n",
    "\n",
    "# Обрахунок тенденції трафіку\n",
    "# Виявлення нових записів\n",
    "new_records = df_version2.join(\n",
    "    df_version1,\n",
    "    on=[\"site_id\", \"traffic_source\"],\n",
    "    how=\"leftanti\"\n",
    ")\n",
    "print(\"Нові записи у версії 2:\")\n",
    "new_records.show()\n",
    "\n",
    "# Порівняння існуючих записів\n",
    "existing_records = df_version2.join(\n",
    "    df_version1,\n",
    "    on=[\"site_id\", \"traffic_source\"],\n",
    "    how=\"inner\"\n",
    ").select(\n",
    "    df_version2[\"site_id\"],\n",
    "    df_version2[\"traffic_source\"],\n",
    "    (df_version2[\"visits\"] - df_version1[\"visits\"]).alias(\"visit_change\"),\n",
    "    round((df_version2[\"avg_duration\"] - df_version1[\"avg_duration\"]), 1).alias(\"avg_duration_change\")\n",
    ")\n",
    "print(\"Тенденція для існуючих записів:\")\n",
    "existing_records.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0e24d2b3-5e90-4a7e-856e-5af5fc49269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
